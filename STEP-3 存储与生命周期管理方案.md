# AWS 存储与生命周期管理方案

## 一、使用 AWS

### 选择 AWS 的原因

在实际生产环境中，通常会使用 AWS 来做业务开发，主要理由：

- **稳定可靠**：AWS 提供成熟的基础设施和高可用能力。
- **降低运维复杂度**：无需自行维护和扩容分布式存储节点等底层设施。
- **完善的生命周期管理**：原生支持对象存储的生命周期规则配置，便于按成本分层管理数据。

---

## 二、Lifecycle Rules（生命周期规则）

### 必要性

**必要：必须启用 Lifecycle 规则。**

#### 理由：成本控制

- 大量日志 / 业务数据通过 Kafka 写入 S3，长期保留会带来较高存储成本。
- 通过生命周期管理，将仅在短期内高频访问的“热数据”和长期低频访问的“冷数据 / 归档数据”分层，可以显著降低整体成本。

---

### 生命周期规则设计

#### 存储策略

- **热数据（S3 Standard）**
    - 保留时长：**3 天**
    - 适用场景：日常业务查询、报表、短期重放等。

- **3 天以后**
    - 不做“Standard → Infrequent Access”之类的中间降级。
    - **直接进入归档层级**：`Glacier Deep Archive`
    - 设计思路：默认情况下，超过 3 天的数据**不再参与日常查询**，仅在极少数需要重放 / 历史追溯时才会恢复。

#### 生命周期规则示意

- **Kafka → S3（Standard）保留 3 天 → 自动转 Glacier Deep Archive → 按预期规则删除**

可以抽象为以下流程：

1. **第 0～3 天：**
    - Kafka 消息通过消费程序写入 **S3 Standard** 存储。
    - 日常业务查询、统计、报表均只依赖这 3 天内的数据。

2. **第 3 天之后：**
    - Lifecycle 规则自动将对象从 **S3 Standard** 转为 **Glacier Deep Archive**。
    - 不再作为在线查询的数据源，仅作为“归档备份”。

3. **更长时间后：**
    - 可根据业务合规与审计要求，设置在 Glacier Deep Archive 中保留的总时长。
    - 到期后由 Lifecycle 规则自动彻底删除，释放存储成本。

---

### 归档数据恢复策略

当需要恢复历史数据（例如排查历史问题、重算某日指标）时，流程如下：

1. **按天目录恢复文件**
    - 针对某一天（或某个时间段）需要重放的数据，从 **Glacier Deep Archive** 发起恢复。
    - 使用 S3 按路径前缀（如 `s3://bucket/path/dt=YYYY-MM-DD/`）选择对应日期目录。

2. **恢复完成后**
    - 将恢复出的文件重新作为 Kafka 重放源，或直接作为离线计算输入。
    - 执行相应的 **业务重跑脚本**：
        - 重算指标
        - 重建某些宽表 / 中间表
        - 重新生成报表

3. **重跑完成**
    - 若无需继续保留这些已恢复的对象，可再次通过脚本或 Lifecycle 策略进行清理，避免重复占用 Standard 存储。

---

## 三、整体流程概览

```text
Kafka → S3 Standard（保留 3 天）
      → 自动转存至 Glacier Deep Archive（长期归档）
      → 按需从 Deep Archive 恢复指定日期数据
      → 重跑业务脚本 / 指标计算 / 报表生成
      → （可选）恢复数据再清理，继续走生命周期
